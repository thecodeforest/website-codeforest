---
title: "Establishing Causality with Counterfactual Prediction"
author: "Mark LeBoeuf"
date: '2017-08-05T21:13:14-05:00'
summary: Sometimes a controlled experiment isn’t an option yet you want to establish
  causality. This post outlines a method for quantifying the effects of an intervention
  via counterfactual predictions.
tags: ["R", "Counterfactual Prediction", "Forecasting", "Experimentation", "Causal Impact"]
categories: ["R", "Counterfactual Prediction", "Forecasting", "Experimentation", "Causal Impact"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, out.height="800px", out.width="800px"}
knitr::include_graphics("/Users/mlebo1/Desktop/site/content/post/images/australia.gif")
```

### Overview

Businesses have several “levers” they can pull to boost sales. For example, a series of advertisements might be shown in a city or region with the hopes of increasing sales for the advertised product. Ideally, the cost associated with the advertisement would be outstripped by the expected increase in sales, visits, conversion, or whatever KPI (key performance indicator) the business hopes to drive. But how do you capture the ROI of the advertisement? In a parallel universe, where all experiments are possible (and unicorns roam the land), we would create two copies of the same city, run the advertisement in one of them, track our metric of interest over the same time period, and then compare the difference between the two cities. All potential confounding variables, such as differences in seasonal variation, viewership rates, or buying preference, would be controlled. Thus, any difference (lift) in our KPI could be attributed to the intervention.

### Souvenir Sales, Australia, and Wacky Inflatable Tube Men

We can’t clone cities, but there is a way to statistically emulate the above situation. This post provides a general overview of how to generate a **counterfactual prediction**, which is a way to quantify what would’ve happened had we never run the advertisement, event, or intervention.

Incremental Sales, ROI, KPIs, Lift – these terms are somewhat abstract. Australia, Souvenirs, Wacky- Inflatable-Tube-Men (WITM for short) – that all seems pretty straight-forward. So imagine you run a souvenir shop near the beach in Australia and you’re looking for ways to attract additional foot traffic and (hopefully) increase sales. You decide to purchase this guy below, hoping it will draw attention to your store, increase foot traffic, and produce more souvenir sales.

```{r, echo=FALSE, out.height="800px", out.width="800px"}
knitr::include_graphics("/Users/mlebo1/Desktop/site/content/post/counterfactual_prediction_images/wacky_inflatable_man.jpg")
```

Being a savvy business owner, you’re interested in the effect of your new WITM on sales. Does having this form of advertisement drive sales beyond what would’ve happened in the absence of any WITM? You look to the other six souvenir shops in separate parts of town to answer this question. The other souvenir shops will serve as a control group because they don’t have a WITM. Additionally, the other shops are located far enough away from your shop so there is no way for their sales to be affected by your WITM (this is a critical assumption that in real-world contexts should be verified). After accounting for baseline differences in sales between your shop and the control shops, you build a forecast – or counterfactual prediction – premised on what would’ve happened in your shop had the WITM intervention never taken place. This prediction will then serve as the baseline from which to compare what happened to sales.

In short, you (the shop owner) are faced with two tasks:

* **Identifying which of control shop(s) are most like your shop**
* **Generating the counterfactual prediction**

I’ll go through each of these steps at a high level, just to give you a general idea of the logic underlying the process. For an excellent, more in-depth explanation of the technical details check out this [post](http://multithreaded.stitchfix.com/blog/2016/01/13/market-watch/) by the author of the `MarketMatching` package, which we will leverage to address both tasks.

### Creating the Synthetic Data Set

Let’s first download the dataset and load up the required libraries. You can find the dataset here.
```{r, include = FALSE}
libs = c("devtools", "CausalImpact", "forecast",
         "data.table", "kableExtra","dplyr", 
         "forcats", "MarketMatching", "knitr", 
         "ggplot2", "ggforce")
lapply(libs, require, character.only = TRUE)
# Uncomment if you dont have the package installed 
#devtools::install_github("klarsen1/MarketMatching", build_vignettes=TRUE)
starting_date = as.Date("1987-01-01")
souvenir = fread("https://robjhyndman.com/tsdldata/data/fancy.dat",
                 data.table = FALSE) %>% 
           rename(sales = V1) %>% 
           mutate(date = seq(starting_date, by = "month", length.out = n()))
```

```{r, eval = FALSE}
libs = c("devtools", "CausalImpact", "forecast",
         "data.table", "kableExtra","dplyr", 
         "forcats", "MarketMatching", "knitr", 
         "ggplot2", "ggforce")
lapply(libs, require, character.only = TRUE)
# Uncomment if you dont have the package installed 
#devtools::install_github("klarsen1/MarketMatching", build_vignettes=TRUE)
starting_date = as.Date("1987-01-01")
souvenir = fread("https://robjhyndman.com/tsdldata/data/fancy.dat",
                 data.table = FALSE) %>% 
           rename(sales = V1) %>% 
           mutate(date = seq(starting_date, by = "month", length.out = n()))
```

The dataset is total monthly souvenir sales for your shop, starting in January of 1987 and running through December of 1993. Let’s take a high-level glance at our time series to see if we need to clean it up.

```{r, fig.width = 10, fig.height = 8, message=FALSE, warning=FALSE}
my_plot_theme = function(){
    font_family = "Helvetica"
    font_face = "bold"
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = "top",
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}

ggplot(souvenir, aes(x = date, y = sales)) + geom_point(size = 2) + 
                                             geom_line(size = 2) + 
  theme_bw() + 
  my_plot_theme()
```

Notice that the seasonal changes become more volatile across time, particularly in the past couple years. The forecasting model we’ll be using is additive and thus cannot account for a multiplicative time series, so we’ll need to log-transform sales before we build a model.

```{r}
souvenir_ts = ts(log(souvenir$sales),
                 frequency = 12,
                 start = c(1987, 1))
```

Now the seasonal fluctuations are roughly constant across time and can be described with an additive model. Next, we'll generate a 6-month ahead forecast. To simulate the positive effect that the WITM intervention had on sales during the 6-month intervention period, we'll sample from a normal distribution where the mean is 5% of the forecasted value and the standard deviation is defined by the residuals of the fitted model. Using the residuals from the fitted model will result in a smaller standard deviation than using a holdout set, but to keep things simple we'll estimate uncertainty based on model residuals. 

```{r}
set.seed(123)
# define length of intervention
trial_period_months = 6
# define simulated effect size as a percentage (e.g., a 5% lift above what is expected each month)
effect_size = 0.05
arima_fit = auto.arima(souvenir_ts)
simulated_intervention = forecast(arima_fit, trial_period_months)$mean
# create simulated effect 
simulated_intervention = sapply(simulated_intervention, function(x) x + rnorm(1, 
                                                     mean = (x * effect_size),
                                                     sd = sd(arima_fit$residuals)))

monthly_log_sales = c(souvenir_ts, simulated_intervention)

study_dates = seq(max(souvenir$date), 
                  by = "month", 
                  length.out = (trial_period_months + 1))

study_dates = study_dates[2:length(study_dates)]
intervention_start = study_dates[1]

souvenir_sim = data.frame(measurement_date = c(souvenir$date, study_dates),
                          sales = c(souvenir_ts, simulated_intervention),
                          store = "1") %>% 
                mutate(time_period = ifelse(measurement_date >= intervention_start, 
                                            "post-intervention",
                                            "pre-intervention"))
```

Let's plot the results to get a better idea of when each of the events is happening.
```{r, fig.width = 10, fig.height = 8, message=FALSE, warning=FALSE}
ggplot(souvenir_sim, aes(x = measurement_date, 
                         y = sales,
                         color = time_period
                         )) + 
  geom_point(size = 2) + geom_line(size = 2) +
  theme_bw() +
  my_plot_theme() + ylab("Log Sales") + xlab("Date") + 
  facet_zoom(x = measurement_date %in% as.Date(souvenir_sim %>%
                                      filter(time_period == "post-intervention") %>%
                                      pull(measurement_date))) + 
  theme(legend.title=element_blank())
```

The blue region is the time before the intervention, while the red region is the time during the intervention. It’s hard to determine if the level (mean) has increased for the last six months of our time series. The uptick could be due to seasonal variation or where the trend of the series was naturally going (Obviously, we know this to be false). We want to rule these explanations out. To do so, we’ll make a counterfactual prediction based on the other control shops. We’ll use the original time series as a basis to create six other time series. Three will be the original time series with an added Gaussian error term while the other three will be random walks. All six will serve as initial candidates for our control shops. Let’s generate the time series for our comparison shops.

```{r, message=FALSE, warning=FALSE}
set.seed(123)
comparison_df = data.frame(NULL)
similiar_stores = c("2", "3", "4")
# generate time series for similar stores
for(i in 1:length(similiar_stores)){
  temp_ts = souvenir_ts
  
  # add a bit of random noise
  temp_ts = data.frame(sales = temp_ts + rnorm(length(temp_ts), 
                                               0, 
                                               (sd(temp_ts) * 0.25)))
  
  # fit a arima model to the simulated sales data
  temp_fit = auto.arima(ts(temp_ts$sales, frequency = 12, 
                           start = c(1987, 1)))
  
  # generate forecast
  forecasted_values = data.frame(forecast(temp_fit, h = trial_period_months))$Point.Forecast
  
  temp_ts = data.frame(sales = c(temp_ts$sales, forecasted_values),
                       store = similiar_stores[i])
  
  comparison_df = bind_rows(comparison_df, temp_ts)
}
# generate time series for random stores
random_stores = c("5", "6", "7")

for(r in random_stores){
  temp_random = data.frame(sales = rnorm((length(souvenir_ts) + trial_period_months), 
                                         mean = mean(souvenir_ts), 
                                         sd = sd(souvenir_ts)),
                           store = r
  )
  comparison_df = rbind(comparison_df, temp_random)
}

comparison_df = comparison_df %>% 
                mutate(measurement_date = rep(seq(starting_date,
                                    by = "month",
                                    length.out = nrow(souvenir_sim)
                                    ),
                                length(unique(comparison_df$store)))
                       ) %>% 
                bind_rows(souvenir_sim %>% 
                          dplyr::select(-time_period))
```

We now have all of time series in a neat dataframe. Let’s visualize what that looks like.

```{r, fig.width = 10, fig.height = 8, message=FALSE, warning=FALSE}
ggplot(comparison_df, aes(x = as.Date(measurement_date), 
                          y = sales, 
                          color = store)) + geom_point() + geom_line() +
  facet_wrap(~store, ncol = 1) + 
  theme_bw() + 
  my_plot_theme() + ylab("Log Sales") + 
  xlab("Date") + 
  theme(strip.text.x = element_text(size = 14, face = "bold"),
        axis.text.y = element_blank(),
        legend.position = "none")
```

Store 1 is our store, while Stores 2-7 are the potential control stores.To make inferences about the effect of our intervention, we want to identify a separate  store(s) with similar sales history to serve as the control store(s). We’ll keep it simple here and only use a single store, but you could use any number of stores as a control. I’ll discuss in the next section how we go about defining and identifying similarity.

### Selecting a Control Time Series with Dynamic Time Warping

We’ll implement a technique known as Dynamic Time Warping. It sounds like something you’d hear in a Steven Hawking TED talk, but it’s just a way to measure the similarity between two sequences. A common approach for measuring the similarity between sequences is to take the squared or absolute difference between them at the same period and then sum up the results. The main drawback with this approach is that it fails to account for seasonal changes that might be shifted or delayed but still occur in the same order. It’s like if two people said the same sentence but one person said it much slower than the other. The order and content of the utterance is the same but the cadence is different. This is where Dynamic Time Warping shines. It stretches or compresses (within some constraints) one time series to make it as similar as possible to the other. An individual’s speech cadence wouldn’t affect our ability to determine the similarity between two separate utterances.

In the current context, we aren’t concerned with leading or lagging seasonality, as a random error was added to each value in the absence of any forward or backward shifts. However, this can be an issue when dealing with phenomena that are impacted by, say, weather. For example, imagine you sell a seasonal product like ice cream. Ice cream sales go up when the weather gets hot, and perhaps it warms up in some parts of the same region before others. This is a case when you might see the exact same sales patterns but some emerge a few months before or after others. Therefore, it is important to use a matching method that can account for these shifts when comparing the similarity of two time-series.

We’ll leverage the `MarketMatching` package to select our control time series. The selection process is done via DTW. 

```{r}
start = min(comparison_df$measurement_date)
end = unique(comparison_df$measurement_date)[(length(unique(comparison_df$measurement_date)) - 
                                                                                       trial_period_months)]
most_similar_store = MarketMatching::best_matches(data = comparison_df,
                             id_variable = "store",
                             date_variable = "measurement_date",
                             matching_variable = "sales",
                             warping_limit = 3,
                             matches = 1,
                             start_match_period = start,
                             end_match_period = end
                             )
```


```{r, echo=FALSE}
kable(most_similar_store$BestMatches, "html",  align = "c") %>%
  kable_styling(bootstrap_options = c("striped",
                                      "hover"),
                full_width = FALSE) %>%
  scroll_box(width = "720px", height = "310px")
```

That was too easy! This table indicates that the pre-intervention sales for Store Number 2 are the most similar to those in Store Number 1. Thus, we’ll use Store 2 to generate our counterfactual prediction.

### Generating Counterfactual Predictions

This is the inference part – namely, can we infer that our intervention impacted sales? 

```{r, message=FALSE, warning=FALSE, include = FALSE}
results = MarketMatching::inference(matched_markets = most_similar_store,
                                     test_market = "1",
                                     end_post_period = max(comparison_df$measurement_date)
                                    )
```

```{r, message=FALSE, warning=FALSE, eval = FALSE}
results = MarketMatching::inference(matched_markets = most_similar_store,
                                     test_market = "1",
                                     end_post_period = max(comparison_df$measurement_date)
                                    )
```
```{r}
## ------------- Inputs -------------
## Test Market: 1
## Control Market 1: 2
## Market ID: store
## Date Variable: measurement_date
## Matching (pre) Period Start Date: 1987-01-01
## Matching (pre) Period End Date: 1993-12-01
## Post Period Start Date: 1994-01-01
## Post Period End Date: 1994-06-01
## Matching Metric: sales
## Local Level Prior SD: 0.01
## Posterior Intervals Tail Area: 95%
##
## 
## ------------- Model Stats -------------
## Matching (pre) Period MAPE: 1.47%
## Beta 1 [2]: 0.9239
## DW: 1.93
##
##
## ------------- Effect Analysis -------------
## Absolute Effect: 2.62 [1.29, 3.99]
## Relative Effect: 4.41% [2.17%, 6.73%]
## Probability of a causal impact: 99.8996%
```

Let’s break down the **Effect Analysis** portion of the output. The **Absolute Effect** indicates a lift of 2.62 in sales over the 6-month intervention period. Given that we're working with logged units, this isn't very helpful without transforming it first. The **Relative Effect**, in contrast, provides a standardized view. Recall that we simulated a 5% lift, and in this case, we observed an estimated lift of 4.41%. Finally, the probability of a causal impact indicates that the likelihood of observing this effect by chance is extremely low (e.g., 100 - 99.8). We can see what was just described above in visual format below. 

```{r, fig.width = 10, fig.height = 8, message=FALSE, warning=FALSE}
plot(results$PlotActualVersusExpected)
```

This drives home that our intervention had a clear effect -- and we should promote the WITM to store manager for the excellent work in driving up sales. Hopefully, this post gave you a better idea of how to quantify the impact of an intervention. The ability to run a randomized experiment in the real-world is often too expensive or simply not possible. In many cases creating a statistical control group is the best (and cheapest) option available. For more information about the forecasting methodology used produce counterfactual predictions, check out the [bsts](https://cran.r-project.org/web/packages/bsts/bsts.pdf) and (CausalImpact)[https://google.github.io/CausalImpact/CausalImpact.html] packages. Happy experimenting!