---
title: "Time Series Forecasting with Gradient Boosting"
author: "Mark LeBoeuf"
date: '2017-11-14T21:13:14-05:00'
summary: "Advanced machine learning algorithms like Gradient Boosting Machines (GBM) can't model time-dependent data without some pre-processing. The additional processing hurdle often deters forecasters from implementing advanced methods in favor of classic (but less powerful) methods, such as ARIMA or Holt-Winters. However, I've observed some notable accuracy gains applying GBM to forecasting problems. Accordingly, this post provides a playbook for data cleaning, feature engineering, model selection, prediction, and risk assessment when leveraging GBM for long-range forecasting."

tags:
- R
- GBM
- Forecasting
- H2O
- Quantile Regression
categories:
- R
- GBM
- Forecasting
- H2O
- Quantile Regression
---

```{r, echo=FALSE, out.height="800px", out.width="800px"}
knitr::include_graphics("/Users/mlebo1/Desktop/site/content/post/images/city_lights.jpg")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Overview

When considering classic prediction problems, some of the following examples come to mind: 

* **Predicting the content of an image**
* **Classifying Fraudulent Transactions**
* **Product recommendations**
* **Illness diagnostics**

A common assumption underlies all of these prediction scenarios: That the relationship between the inputs and output remain constant over time. For example, the appearance of dogs or cats hasn't changed in recent history and will likely remain the same for many years to come. Thus, in an image prediction task, a model trained with images of dogs and cats from the year 2016 could perform just as well when fed images from the year 2020 because the appearance of dogs and cats will not change much from year to year.

Now let's think about classic time series prediction problems, with a few examples listed below: 

* **Daily temperature for the next 10 days**
* **Monthly unemployment rates for 2018 in the United States**
* **Daily demand for flights between Portland and New York City in December**

The difference between time series prediction problems and those listed initially is that the relationship between our input and outputs are likely changing over time. That is, we have to account for and anticipate where the thing we're trying to predict might go in the future. For instance, take the last example -- predicting daily bookings for flights from Portland to NYC. Has the number of people flying this route increased or decreased over the past few years? Has there been a change in the overall number of people flying in general? Has the introduction of price-comparison websites shifted the time periods during which people book flights? These forces may change consumption patterns over time, so a portion of our data may no longer reflect the relationship between the inputs and output. While this presents a more challenging prediction scenario, all of these forces can be accounted for and modeled. And once they are, a time series prediction problem is no different than any other prediction problem. Let's make these concepts more concrete by working through an example. 

### Forecasting Power Consumption

Nothing gets me more *charged up* than forecasting electricity consumption, so the data we'll use here is a time series of consumption for an anonymized commercial building from 2012. Measurements were recorded for a single year at five-minute intervals, so each hour has 12 readings, and each day has 288 readings. Our goal is to train a model on several months of consumption data and then produce a one-week-ahead forecast for anticipated consumption. Let's get started by downloading the data and examining the first few rows. 

```{r, include = FALSE}
libs = c('data.table','h2o','forecast',
         'lubridate','forcats',
          'ggforce', 'ggplot2',
          'reshape', 'knitr',
         'kableExtra',
         'dplyr')
lapply(libs, require, character.only = TRUE)
data_source = 'https://open-enernoc-data.s3.amazonaws.com/anon/csv/832.csv'
df_elec = fread(data_source,
                data.table = FALSE)
```

```{r, eval = FALSE}
libs = c('data.table','h2o','forecast',
         'lubridate','forcats',
          'ggforce', 'ggplot2',
          'reshape', 'knitr',
         'kableExtra',
         'dplyr')
lapply(libs, require, character.only = TRUE)
data_source = 'https://open-enernoc-data.s3.amazonaws.com/anon/csv/832.csv'
df_elec = fread(data_source,
                data.table = FALSE)
```

```{r, echo = FALSE}
kable(head(df_elec, 15), "html", align = "c") %>%
  kable_styling(bootstrap_options = c("striped", 
                                      "hover"),
                full_width = TRUE) %>% 
  scroll_box(width = "720px", height = "400px")
```

We only need the date-time and electricity values, so we'll filter several variables and extract the date information from the timestamps. 

```{r, message=FALSE, warning=FALSE}
df_elec = df_elec %>% 
  dplyr::select(-anomaly, -timestamp) %>%
  dplyr::rename(date_time = dttm_utc) %>%
         mutate(date_only = as.Date(date_time)) %>% 
         mutate(month = lubridate::month(date_time, label = TRUE),
                week = as.factor(lubridate::week(date_time)),
                hour = lubridate::hour(date_time),
                day = lubridate::day(date_time))

```

To keep things really simple, we'll model the average reading per hour instead of every five minutes, meaning there will be 24 readings per day instead of 288.  

```{r, message=FALSE, warning=FALSE}
df_hourly = df_elec %>% 
         group_by(date_only, month, hour) %>% 
         summarise(value = mean(value)) %>% 
         ungroup() %>% 
         mutate(hour = ifelse(nchar(as.character(hour)) == 1, 
                              paste0("0", as.character(hour)),
                              hour)) %>% 
         mutate(hour = paste(hour, "00", "00", sep = ":")) %>% 
         mutate(date_time = lubridate::ymd_hms(paste(date_only, hour))) %>% 
         dplyr::select(date_time, month, value) %>% 
         mutate(week = as.factor(lubridate::week(date_time)),
                day = lubridate::day(date_time),
                hour = lubridate::hour(date_time)
                ) 
```

Next, we'll filter the timeframe to only a few months to speed up the eventual training time and break our dataset into *training* and *testing* segments. We'll hold out the last week for testing, which contains a total of 168 observations (24 per day). 

```{r}
df_hourly = df_hourly %>% 
            filter(month %in% c('Feb','Mar', 'Apr', 'May', 'Jun')) %>% 
            dplyr::select(-month)
# daily period (24 hours)
daily_p = 24
# weekly period (7 days)
weekly_p = 7
# hold out last week of time series for testing
train_df = df_hourly[1:(nrow(df_hourly) - daily_p * weekly_p),]
test_df = tail(df_hourly, daily_p * weekly_p)
```

Let's start by creating a high level view of the entire time series and loading up a custom plotting theme. 
```{r fig.width = 10, fig.height = 8}
my_plot_theme = function(){
    font_family = "Helvetica"
    font_face = "bold"
    return(theme(
    axis.text.x = element_text(size = 18, face = font_face, family = font_family),
    axis.text.y = element_text(size = 18, face = font_face, family = font_family),
    axis.title.x = element_text(size = 20, face = font_face, family = font_family),
    axis.title.y = element_text(size = 20, face = font_face, family = font_family),
    strip.text.y = element_text(size = 18, face = font_face, family = font_family),
    plot.title = element_text(size = 18, face = font_face, family = font_family),
    legend.position = "top",
    legend.title = element_text(size = 16,
    face = font_face,
    family = font_family),
    legend.text = element_text(size = 14,
    face = font_face,
    family = font_family)
))
}

bind_rows(train_df %>% 
            mutate(part = "train"),
          test_df %>% 
            mutate(part = "test")) %>% 
  dplyr::select(date_time, part, value) %>% 
  ggplot(aes(x = date_time, y = value, color = part)) + 
  geom_line() +   
  facet_zoom(x = date_time %in% c(test_df$date_time)) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab("Date-Time") + 
  ylab("Value") + 
  theme(legend.title=element_blank())
```

There is clear seasonality in the data -- and that's what we'll focus on next. The measurements are at the hourly level, so a periodicity (or seasonality) of 24 is likely. This makes sense: consumption increases during business hours and decreases after-hours. It's also possible there is a daily component, in that consumption is higher on weekdays relative to weekends. However, we have no idea what kind of business we're working with here (since all the data is anonymized) and can't make any assumptions about the nature of consumption throughout the week. Rather than assuming these patterns exist, I find it helps to sample some days and plot them out. If there is a pattern amongst the sampled values, then there's a signal we can leverage to make a forecast. Let's examine how consumption varies throughout the day and week for a random sample of 100 days. 

```{r ,  message=FALSE, warning=FALSE}
set.seed(123)
sample_size = 100
sample_days = train_df %>% 
  dplyr::select(week, day) %>% 
  distinct() %>% 
  sample_n(sample_size) %>% 
  inner_join(train_df) %>% 
  mutate(day_of_week = lubridate::wday(date_time, 
                                       label = TRUE))
```

```{r fig.height = 8, fig.width=10, message=FALSE, warning=FALSE}
ggplot(sample_days, aes(x = hour, y = value, color = day_of_week)) + 
  geom_point() + 
  geom_jitter()  + 
  stat_smooth(se = FALSE, size = 2) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab("Hour") + 
  ylab("Value") + 
  facet_grid(day_of_week ~ .) + 
  theme(legend.position = "none", 
  axis.text.y = element_text(size = 13))
```

There is minimal consumption on Saturday and Sunday, while Monday through Friday has higher levels. Monday through Thursday follow a similar "S" shaped pattern throughout the day. Friday starts out similar to the other weekdays but trails off in the later hours of the day. Accordingly, there appears to be an hourly and weekly seasonality component that we can model. In the following section, we'll go over how to translate these components into features.

### Feature Engineering

Based on the initial exploratory analysis, we've identified two seasonal components. We'll capture this via `msts`, which is intended for time series data with multiple seasonal periods. 

```{r}
data_ts = msts(train_df$value, seasonal.periods = c(daily_p,
                                                    daily_p * weekly_p))
```

We'll take our `msts` object and extract the seasonal features via a **Fourier Transformation**. Fourier transformations are pretty incredible, and they have lots of real-world applications. In a nutshell, applying a Fourier transformation to a time-series (or any signal) is a way of "deconstructing" the series into different parts, each with a unique frequency. If all the parts are added back together, we get the original series. Our goal is to take the fewest number of parts that capture the greatest amount of variation when we add them back together. Let's visualize what this looks like by examining the forecasted seasonal component for the test week. 

```{r}
K = 2
fourier_train = data.frame(fourier(data_ts, K = c(K, K)))
fourier_test_fcast <- data.frame(fourier(data_ts, 
                                         K = c(K, K), 
                                         h = daily_p * weekly_p))
```


```{r fig.height = 8, fig.width=10, message=FALSE, warning=FALSE}
cbind(fourier_test_fcast,
      data.frame(date_time = test_df$date_time)) %>%
melt(id.vars = "date_time") %>%
ggplot(aes(x = date_time, y = value, color = variable)) +
  geom_line() +
  facet_grid(variable ~ .) +
  theme_bw() +
  my_plot_theme() + 
  xlab("Date") + 
  ylab("Value") +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 13),
        strip.text.y = element_text(size = 13))
```

Note how all the features with *24* capture the hourly component, while those with a *168* capture the daily component. Note also the continuous nature of the waves. Indeed, one clear advantage of using this approach for modeling seasonality is that it produces **smooth** forecasts. If we look back at the plot of the daily consumption across the week, the hour-to-hour, day-to-day, and week-to-weekend changes are gradual. If we modeled the hour and day features as dummy variables, simply taking the average for each day or hour, then our seasonality would be stepwise and **not smooth**, which would likely produce sub-optimal forecasts (also **not smooth**).

Next, we'll extract the *trend* component from the time series. We want to remove this part, and the reasoning is simple: Our model can't account for changes that occur over time. Let's extract the trend, generate a trend-only forecast, and later on, we'll add the trend-only forecast back in. 

```{r}
ts_input = ts(data_ts, freq = daily_p * weekly_p)

# deconstruct series into seasonal, trend, and error components
decomp_ts = stl(ts_input, s.window = "periodic", robust = TRUE)

# add together the seasonal and error component
seasonal_error_part = apply(data.frame(decomp_ts$time.series)[,c(1, 3)], 1, sum)

# extract the seasonal component
seasonal_part = as.vector(decomp_ts$time.series[,1])

# extract the trend component
trend_part = ts(decomp_ts$time.series[,2])

# generate a forecast for the trend 
trend_forecast_test = as.vector(forecast(auto.arima(trend_part), 
                          h = nrow(test_df))$mean)
```

Now we'll work with the `seasonal_error_part`, which is the sum of our seasonal component and error component. In this section we'll create a *seasonal lag*, which is a simple (but effective) way of summarizing recent seasonal variation.

```{r}
seasonal_train = seasonal_part[1:(nrow(train_df) - (weekly_p * daily_p))]
seasonal_test = tail(seasonal_part, weekly_p * daily_p)
# build the training set
train_input = data.frame(value = head(seasonal_error_part, length(seasonal_train)),
                         fourier_train[1:length(seasonal_train),],
                         seasonal_lag = seasonal_train
                         )
# build the testing set
test_input = data.frame(value = test_df$value,
                        fourier_test_fcast,
                        seasonal_lag = seasonal_test
                        )
```

Our time-series has successfully been translated into the format required for forecasting.

### Forecasting & Prediction Intervals

We'll use the `h2o` package to generate forecasts and prediction intervals. A forecast is our best guess for what we believe the future holds. But that's only one part of the process. The other part -- a prediction interval -- captures our certainty surrounding that guess. Prediction intervals can be created with a technique called *quantile regression*. Quantile regression works by differentially weighting errors during the model fitting process. It models the relationship between our inputs and a specific quantile (i.e., percentile) of our output. For example, if the goal is to model the relationship between X and Y at the 90th percentile of Y, 10% of the errors should be positive and 90% negative to minimize our loss function (e.g., MAE, MSE). In this case, we'll generate forecasts at the following percentiles: 

* **10%**
* **50%**
* **90%**

The 50th percentile is our forecast, while the 10th and 90th percentile will serve as the lower and upper bound of our 80 percent prediction interval. If our prediction interval is functioning properly, then approximately 90% of the errors for the 10th percentile should be positive, while 10% of the errors for 90% percentile should be positive. We can verify this assumption by examining the residuals on the test set. I've created a function below to do this in `h2o`.

```{r }
# function to generate quantile regression estimates
gbm_prediction_interval = function(x_var, y_var, train_input, test_input, quantiles, seed){
  pred_interval = list()
  for(q in quantiles){
  quantile_fit = h2o.gbm(x = x_var,
                           y = y_var,
                           training_frame = as.h2o(train_input),
                           distribution = "quantile",
                           quantile_alpha = q,
                           stopping_rounds = 15,
                           ntrees = 5000,
                           seed = seed
                           )
    quantile_pred = as.vector(predict(quantile_fit, as.h2o(test_input)))
    pred_interval[[paste0("q_", q)]] = quantile_pred
  }
  return(pred_interval)
}
```
```{r, message=FALSE, warning=FALSE, include = FALSE}
h2o.init()
# specify 80% prediction interval 
quantiles = c(0.1,0.5,0.9)
# set seed for reproduceability
seed = 123
y_var = "value"
x_var = setdiff(names(train_input),y_var)
detrended_consumption = gbm_prediction_interval(x_var, 
                                                y_var, 
                                                train_input,
                                                test_input, 
                                                quantiles, 
                                                seed)
# add trend forecast back to predictions 
predicted_consumption = data.frame(lapply(detrended_consumption, 
                                          function(x) x + trend_forecast_test)) %>% 
                        mutate(value = test_df$value)

```

```{r, message=FALSE, warning=FALSE, eval = FALSE}
h2o.init()
# specify 80% prediction interval 
quantiles = c(0.1,0.5,0.9)
# set seed for reproduceability
seed = 123
y_var = "value"
x_var = setdiff(names(train_input),y_var)
detrended_consumption = gbm_prediction_interval(x_var, 
                                                y_var, 
                                                train_input,
                                                test_input, 
                                                quantiles, 
                                                seed)
# add trend forecast back to predictions 
predicted_consumption = data.frame(lapply(detrended_consumption, 
                                          function(x) x + trend_forecast_test)) %>% 
                        mutate(value = test_df$value)

```

We now have our final model and predictions. Before considering performance on the test set, I'll reflect on something that I wondered the first time I went through all of the steps outlined above: Is it worth the time and trouble? What kind of performance would result if we avoided all the lags, transformations, and assorted data wizardry and did something like this:

```{r}
arima_fit = auto.arima(data_ts)
arima_fcast = forecast(arima_fit, h = nrow(test_df))$mean
```

That was two lines. Boom. Done. The ARIMA forecast will serve as our performance benchmark. Keep in mind that ARIMA models the **linear** relationship between inputs and outputs. There are many instances in which capturing higher-order interactions and non-linearities can greatly improve forecasting accuracy, and we'll determine if this happens to be one of those instances. 

```{r, message=FALSE, warning=FALSE}
# combine all forecasts here
perf_df = test_df %>% 
          mutate(arima_fcast = arima_fcast,
                 gbm_fcast = predicted_consumption$q_0.5,
                 lwr = predicted_consumption$q_0.1,
                 upr = predicted_consumption$q_0.9
                 ) %>% 
          dplyr::select(-week, -day, -hour) %>% 
          dplyr::rename(actual = value) %>% 
          data.frame()
```

```{r fig.height = 7, fig.width = 10}
perf_df %>% 
  melt('date_time') %>% 
  filter(variable %in% c("actual", "arima_fcast", "gbm_fcast")) %>% 
  ggplot(aes(x = date_time, y = value, color = variable)) + 
  geom_line(size = 1) + 
  theme_bw() + 
  my_plot_theme() + 
    xlab("Date-Time") + 
    ylab("Value") + 
  theme(legend.title=element_blank())
```

In this case, the difference between the two methods is remarkable: GBM substantially outperforms the ARIMA model. While the granular nature of the data are particularly suited for this forecasting approach, I've found you can *boost* your accuracy substantially by modeling non-linearities with more advanced machine learning methods. It's obviously more involved than running a one-liner, and scalability can become an issue when generating many forecasts, but it can pay huge dividends in terms of accuracy.

So now that we have our initial forecast, let's examine our prediction interval. 

```{r fig.height = 8, fig.width = 10}
perf_df %>% 
  ggplot(aes(x = date_time, y = actual)) + 
  geom_ribbon(aes(ymin = lwr,
                  ymax = upr),
              fill = "#F8766D",
              alpha = 0.7,
              linetype = 2,
              size = 2
              ) + 
  geom_line(size = 2
            ) + 
  theme_bw() + 
  my_plot_theme() + 
  xlab("Date-Time") + 
  ylab("Value")
  
```

The black line is the actual consumption values, while the shaded red area is our prediction interval. Overall the interval does a decent job of capturing uncertainty surrounding the forecasts, but let's verify the distribution of our residuals. Are ~90% of errors above the 10th quantile forecast? 

```{r, eval = FALSE}
print(round(sum(ifelse(perf_df$actual - perf_df$lwr >= 0, 1, 0))/nrow(perf_df) * 100, 1))
```
```{r}
## 89.3
```

Looks good! Almost 90% exactly. What about the upper bound?  

```{r, eval = FALSE}
print(round(sum(ifelse(perf_df$actual - perf_df$upr >= 0, 1, 0))/nrow(perf_df) * 100, 1))
```
```{r}
## 4.2
```

Still reasonably close to 10%. The upper bound is more conservative than it needs to be (i.e., it is too wide in some spots) but it's not too far from the benchmark. Overall our prediction interval performed well. 

### Final Remarks

Ideally, this post clarified one the biggest hurdles faced by data scientists and analysts: Getting your data into the right format to make the best forecast possible. If there is one thing to take away from this post, it's that time series are no different than "regular" machine learning problems. There are a few more pre-processing steps to go through, but in many cases, the extra effort can result in more accurate predictions. Happy Forecasting! 
